{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Modelling Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work book summary:\n",
    "\n",
    "This work book looks at creating a benchmark for the two strategies. I will use a confusion matrix to categorise the results.\n",
    "\n",
    "The presence of a pattern is supposed to indicate a market reversal. Therefore I will assume the pattern has decided to execute a trade. If the market then moves above the threshold it will be classfied as a true positive. If the threshold is not met then it will be recorded as a false positive.\n",
    "\n",
    "Marubuzo: \n",
    "+ Accuracy:\t    0.62\n",
    "+ Precision:\t0.62\n",
    "\n",
    "Fractals: \n",
    "+ Accuracy:\t    0.67\n",
    "+ Precision:\t0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "- [1.0 Marubuzo Model](#1.0-Marubuzo-Model)\n",
    "    - [1.1 Load Data](#1.1-Load-Data)\n",
    "    - [1.2 Get patterns](#1.2-Get-patterns)\n",
    "    - [1.3 Train Test Split](#1.3-Train-Test-Split)\n",
    "    - [1.4 Loop through patterns](#1.4-Loop-through-patterns)\n",
    "    - [1.5 Process results](#1.5-Process-results)\n",
    "\n",
    "- [2.0 Fractal Model ](#2.0-Fractal-Model)\n",
    "    - [2.1 Load Data](#2.1-Load-Data)\n",
    "    - [2.2 Get patterns](#2.2-Get-patterns)\n",
    "    - [2.3 Train Test Split](#2.3-Train-Test-Split)\n",
    "    - [2.4 Loop through patterns](#2.4-Loop-through-patterns)\n",
    "    - [2.5 Process results](#2.5-Process-results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "# from statsmodels.tsa.arima_model import ARIMA, ARMA, ARMAResults, ARIMAResults\n",
    "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from pmdarima import auto_arima\n",
    "# import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Marubuzo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_maru = pd.read_csv('/Users/stuartdaw/Documents/Capstone_data/data/resampled/eur-usddailyMarubozu.csv', \n",
    "                    index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_maru['date+5'] = pd.to_datetime(daily_maru['date+5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2000-07-24 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_maru.loc[daily_maru.index[1],'date+5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(daily_maru['date+5'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Get patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pattern = pd.read_csv('/Users/stuartdaw/Documents/Capstone_data/patterns/dailyMarubozu.csv', \n",
    "                           parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pattern['pattern_end'] = pd.to_datetime(daily_pattern['pattern_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pattern_end   2000-10-20\n",
       "Name: 1, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_pattern.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(daily_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split based on the pattern date +5 more time frames\n",
    "def create_train_test_split(date, time_series, model_info):\n",
    "    test_end_date = time_series.loc[date,'date+5']\n",
    "    \n",
    "    train_test = time_series.loc[time_series.index <= test_end_date]\n",
    "  \n",
    "    target_value = time_series.loc[time_series.index == date,'exit_price'].item()\n",
    "    \n",
    "    train_test.insert(0, 'target_price', target_value)\n",
    "    \n",
    "    model_info['signal'] = time_series.loc[date,'marubozu']\n",
    "    \n",
    "    train_test.insert(0, 'signal', model_info['signal'])\n",
    "    \n",
    "    model_info['start'] = len(train_test)-5\n",
    "    model_info['end'] = len(train_test)-1\n",
    "    \n",
    "    model_info['train'] = train_test.iloc[:model_info['start']]\n",
    "    model_info['test'] = train_test.iloc[model_info['start']:]\n",
    "\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the price cross the threshold?\n",
    "def meet_threshold(row):\n",
    "    if row['signal'] == -1 and row['low'] <= row['target_price']:\n",
    "        return -1\n",
    "    elif row['signal'] == 1 and row['high'] >= row['target_price']:\n",
    "        return 1    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to store outcomes\n",
    "def create_results_outcomes_dataframe(test): #, predictions):    \n",
    "    outcomes = pd.DataFrame()\n",
    "    outcomes['low'] = test['low']\n",
    "    outcomes['high'] = test['high']\n",
    "#     outcomes['preds'] = predictions.values\n",
    "    outcomes['target_price'] = test['target_price']\n",
    "    outcomes['direction'] = test['signal']\n",
    "    outcomes['correct_call'] = test.apply(meet_threshold, axis=1)\n",
    "\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As its the benchmark then it is assumed that that a buy/sell decision is made\n",
    "def classify(outcomes):\n",
    "    \n",
    "    \n",
    "    if max(outcomes['direction']) == 1:\n",
    "        \n",
    "        if max(outcomes['correct_call']) == 0:\n",
    "            return 'fp'\n",
    "        elif max(outcomes['correct_call']) == 1:\n",
    "            return 'tp'\n",
    "        \n",
    "    elif max(outcomes['direction']) == -1:\n",
    "        \n",
    "        if min(outcomes['correct_call']) == 0:\n",
    "            return 'fp'\n",
    "        elif min(outcomes['correct_call']) == -1:\n",
    "            return 'tp'\n",
    "        \n",
    "    else:\n",
    "        return 'ERROR'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Loop through patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loop through the patterns, train test split, determine outcome and. save results\n",
    "\n",
    "model_info = {\"train\":None,\"test\":None,\"start\":None,\"end\":None,\"signal\":None}\n",
    "benchmark_results = []\n",
    "\n",
    "for match in daily_pattern['pattern_end']:\n",
    "    \n",
    "    results_dict = {'name':None,'pattern':None,'date':None,\n",
    "                   'time_frame':None,'RMSE':None,\n",
    "                   'MSE':None, 'classification':None}\n",
    "    \n",
    "    results_dict['name'] = 'Bechmark: ' + str(match)\n",
    "    results_dict['strategy'] = 'Maribozu'\n",
    "    results_dict['time_frame'] = 'daily'\n",
    "\n",
    "    model_info = create_train_test_split(match, daily_maru, model_info)\n",
    "\n",
    "    if len(model_info['train']) < 10:\n",
    "        continue\n",
    "\n",
    "    outcomes = create_results_outcomes_dataframe(model_info['test'])\n",
    "\n",
    "    results_dict['classification'] = classify(outcomes)\n",
    "\n",
    "    benchmark_results.append(results_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the confuson matrix by processing the data\n",
    "def create_cm(results):\n",
    "    \n",
    "    res_cm = [[0,0],\n",
    "              [0,0]]\n",
    "    \n",
    "    for result in results:\n",
    "        res = result['classification']\n",
    "        \n",
    "        if res == 'tp':\n",
    "            res_cm[0][0] += 1\n",
    "        elif res == 'fp':\n",
    "            res_cm[0][1] += 1\n",
    "        elif res == 'fn':\n",
    "            res_cm[1][0] += 1\n",
    "        elif res == 'tn':\n",
    "            res_cm[1][1] += 1\n",
    "    \n",
    "    return res_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save confusion matrix\n",
    "cm = create_cm(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual success</th>\n",
       "      <th>actual non_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred_success</th>\n",
       "      <td>40</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_non_success</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  actual success  actual non_success\n",
       "pred_success                  40                  24\n",
       "pred_non_success               0                   0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display confusion matrix\n",
    "cm_df = pd.DataFrame(cm, index=['pred_success', 'pred_non_success'], columns=['actual success', 'actual non_success'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(cm):\n",
    "    # Accuracy - how many did the model get right\n",
    "    # Total number of correct predictions / total number of predictions\n",
    "    acc= (cm[0][0]+cm[1][1])/(np.sum(cm))\n",
    "    \n",
    "    # Precision proportion of positive identifications that were actually correct\n",
    "    # True positives/ true positives + false positives)\n",
    "    prec = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "    \n",
    "    # Recall - proportion of actual positives that were correctly defined\n",
    "    # True positives/ true positives + false negatives\n",
    "    rec = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "\n",
    "    print(f\"Accuracy:\\t{round(acc,2)}\\nPrecision:\\t{round(prec,2)}\\nRecall:\\t\\t{round(rec,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t0.62\n",
      "Precision:\t0.62\n",
      "Recall:\t\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print_metrics(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Fractal Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_fract = pd.read_csv('/Users/stuartdaw/Documents/Capstone_data/data/resampled/eur-usddailyfractals.csv', \n",
    "                    index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_fract['date+5'] = pd.to_datetime(daily_fract['date+5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Get patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractal_pattern = pd.read_csv('/Users/stuartdaw/Documents/Capstone_data/patterns/dailyfractals.csv', \n",
    "                           parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractal_pattern['pattern_end'] = pd.to_datetime(fractal_pattern['pattern_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "613"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fractal_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train test split based on the pattern date +5 more time frames\n",
    "\n",
    "def create_train_test_split(date, time_series, model_info):\n",
    "    test_end_loc = time_series.index.get_loc(date) + 6\n",
    "\n",
    "    train_test = time_series.iloc[:test_end_loc]\n",
    "\n",
    "    target_value = time_series.loc[time_series.index == date,'exit_price'].item()\n",
    "    \n",
    "    train_test.insert(0, 'target_price', target_value)\n",
    "    \n",
    "    model_info['signal'] = time_series.loc[date,'fractal_end']\n",
    "    \n",
    "    train_test.insert(0, 'signal', model_info['signal'])\n",
    "    \n",
    "    model_info['start'] = len(train_test)-5\n",
    "    model_info['end'] = len(train_test)-1\n",
    "    \n",
    "    model_info['train'] = train_test.iloc[:model_info['start']]\n",
    "    model_info['test'] = train_test.iloc[model_info['start']:]\n",
    "\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine whether the signal is correct and the future price is surpassed\n",
    "def meet_threshold(row):\n",
    "    if row['signal'] == -1 and row['low'] <= row['target_price']:\n",
    "        return -1\n",
    "    elif row['signal'] == 1 and row['high'] >= row['target_price']:\n",
    "#         print(f\"row high: {row['high']} >= row dbl height: {row['target_price']}\" )\n",
    "        return 1    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dateframe of outcomes\n",
    "def create_results_outcomes_dataframe(test):   \n",
    "    outcomes = pd.DataFrame()\n",
    "    outcomes['low'] = test['low']\n",
    "    outcomes['high'] = test['high']\n",
    "    outcomes['5_day_avg'] = test['5_day_avg']\n",
    "    outcomes['open'] = test['open']\n",
    "    outcomes['close'] = test['close']\n",
    "    outcomes['target_price'] = test['target_price']\n",
    "    outcomes['direction'] = test['signal']\n",
    "    outcomes['correct_call'] = test.apply(meet_threshold, axis=1)\n",
    "\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the classfication of the strategy\n",
    "# As its the benchmark then it is assumed that that a buy/sell decision is made\n",
    "\n",
    "def classify(outcomes):\n",
    "    \n",
    "    if max(outcomes['direction']) == 1:\n",
    "        \n",
    "        if max(outcomes['correct_call']) == 0:\n",
    "            return 'fp'\n",
    "        elif max(outcomes['correct_call']) == 1:\n",
    "            return 'tp'\n",
    "        \n",
    "    elif max(outcomes['direction']) == -1:\n",
    "        \n",
    "        if min(outcomes['correct_call']) == 0:\n",
    "            return 'fp'\n",
    "        elif min(outcomes['correct_call']) == -1:\n",
    "            return 'tp'\n",
    "        \n",
    "    else:\n",
    "        return 'ERROR'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Loop through patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop through the patterns, train test split, determine outcome and. save results\n",
    "\n",
    "model_info = {\"train\":None,\"test\":None,\"start\":None,\"end\":None,\"signal\":None}\n",
    "benchmark_results = []\n",
    "\n",
    "for match in fractal_pattern['pattern_end']:\n",
    "\n",
    "    results_dict = {'name':None,'pattern':None,'date':None,\n",
    "                   'time_frame':None,'RMSE':None,\n",
    "                   'MSE':None, 'classification':None}\n",
    "    \n",
    "    results_dict['name'] = 'Bechmark: ' + str(match)\n",
    "    results_dict['strategy'] = 'Maribozu'\n",
    "    results_dict['time_frame'] = 'daily'\n",
    "\n",
    "    model_info = create_train_test_split(match, daily_fract, model_info)\n",
    "\n",
    "    if len(model_info['train']) < 10:\n",
    "        continue\n",
    "\n",
    "    outcomes = create_results_outcomes_dataframe(model_info['test'])\n",
    "    results_dict['classification'] = classify(outcomes)\n",
    "\n",
    "    benchmark_results.append(results_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the confuson matrix by processing the data\n",
    "\n",
    "def create_cm(results):\n",
    "    \n",
    "    res_cm = [[0,0],\n",
    "              [0,0]]\n",
    "    \n",
    "    for result in results:\n",
    "        res = result['classification']\n",
    "        \n",
    "        if res == 'tp':\n",
    "            res_cm[0][0] += 1\n",
    "        elif res == 'fp':\n",
    "            res_cm[0][1] += 1\n",
    "        elif res == 'fn':\n",
    "            res_cm[1][0] += 1\n",
    "        elif res == 'tn':\n",
    "            res_cm[1][1] += 1\n",
    "    \n",
    "    return res_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "cm = create_cm(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual success</th>\n",
       "      <th>actual non_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred_success</th>\n",
       "      <td>410</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_non_success</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  actual success  actual non_success\n",
       "pred_success                 410                 202\n",
       "pred_non_success               0                   0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "cm_df = pd.DataFrame(cm, index=['pred_success', 'pred_non_success'], columns=['actual success', 'actual non_success'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(cm):\n",
    "    # Accuracy - how many did the model get right\n",
    "    # Total number of correct predictions / total number of predictions\n",
    "    acc= (cm[0][0]+cm[1][1])/(np.sum(cm))\n",
    "    \n",
    "    # Precision proportion of positive identifications that were actually correct\n",
    "    # True positives/ true positives + false positives)\n",
    "    prec = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "    \n",
    "    # Recall - proportion of actual positives that were correctly defined\n",
    "    # True positives/ true positives + false negatives\n",
    "    rec = cm[0][0]/(cm[0][0]+cm[1][0])\n",
    "\n",
    "    print(f\"Accuracy:\\t{round(acc,2)}\\nPrecision:\\t{round(prec,2)}\\nRecall:\\t\\t{round(rec,2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t0.67\n",
      "Precision:\t0.67\n",
      "Recall:\t\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Display the results\n",
    "print_metrics(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(daily.loc[daily.index == daily_pattern.loc[10]['pattern_end']].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'daily' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ac40b297295f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdaily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdaily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdaily_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pattern_end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'daily' is not defined"
     ]
    }
   ],
   "source": [
    "daily.loc[daily.index == daily_pattern.loc[10]['pattern_end']].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pattern.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1 date out\n",
    "curr_pattern = daily.loc[daily.index == daily_pattern.loc[10]['pattern_end']].index[0]\n",
    "curr_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_end_date = daily.loc[daily.loc[daily.index == daily_pattern.loc[10]['pattern_end']].index[0],'date+5']\n",
    "test_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily.loc[daily.index == curr_pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = daily.loc[daily.index <= test_end_date]\n",
    "# train_test = daily.loc[daily.index <= '2004-2-28 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily.loc[daily.index <= end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily.loc[daily.index == daily_pattern.loc[10]['pattern_end'],'double_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = daily.loc[daily.index == daily_pattern.loc[10,'pattern_end'],'double_height'].item()\n",
    "target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def choose_exit_price(row, target_price, signal=-1):\n",
    "#     if signal == -1:\n",
    "#         return target_price\n",
    "# #         return row['close'] - (row['height'] * 1)\n",
    "#     else:\n",
    "#         return target_price\n",
    "\n",
    "# #         return row['close'] + (row['height'] * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test['double_height'] = train_test.apply(choose_exit_price, axis=1)\n",
    "# train_test['double_height'] = daily.loc[daily.index == daily_pattern.loc[10,'pattern_end'],'double_height'].item()\n",
    "#train_test.loc['double_height'] = [target_value for x in train_test.loc[:,['double_height']]]\n",
    "train_test.insert(0, 'target_price', target_value)\n",
    "# train_test.insert(0, 'signal', signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = daily.loc[daily.index == daily_pattern.loc[10,'pattern_end'],'marubozu'].item()\n",
    "signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[signal] * (len(train_test)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test.loc[:,['signal']] = [signal] * (len(train_test))\n",
    "# train_test.loc[:,['signal']] = [signal]\n",
    "# df.insert(0, 'A', 'foo')\n",
    "train_test.insert(0, 'signal', signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.tail(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start=len(train)\n",
    "# end=len(train)+len(test)-1\n",
    "start = len(train_test)-5\n",
    "end = len(train_test)-1\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set for testing\n",
    "train = train_test.iloc[:start]\n",
    "test = train_test.iloc[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_plot(train, test):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(train, c='blue')\n",
    "    plt.plot(test, c='orange');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plot confirms that our train test split makes sense\n",
    "train_test_plot(train['close'], test['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima(daily['close'].dropna(), seasonal=False).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(train['low'], order=(0,1,0))\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = results.predict(start=start, end=end, dynamic=False, typ='levels').rename('ARIMA-0-1-0 Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def justified(row):\n",
    "    \n",
    "    if row['signal'] == -1 and row['low'] <= row['target_price']:\n",
    "        return 1\n",
    "    elif row['signal'] == 1 and row['high'] >= row['target_price']:\n",
    "        return 1    \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.DataFrame()\n",
    "outcomes['low'] = test['low']\n",
    "outcomes['high'] = test['high']\n",
    "\n",
    "outcomes['preds'] = predictions.values\n",
    "outcomes['target_price'] = test['target_price']\n",
    "# outcomes['direction'] = test['signal']\n",
    "outcomes['signal_match'] = test.apply(justified, axis=1)\n",
    "\n",
    "#daily_pre['target_price'] = daily_pre.apply(choose_exit_price, axis=1)\n",
    "# outcomes.append(predictions, ignore_index=True)\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions['date']  = test.index\n",
    "#predictions.reset_index(test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.reindex(test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()['close'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()['close'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes['low'].plot(legend=True, figsize=(12,8))\n",
    "outcomes['preds'].plot(legend=True);\n",
    "outcomes['target_price'].plot(legend=True);\n",
    "\n",
    "# predictions.plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error = mean_squared_error(test['close'], predictions)\n",
    "print(f'ARIMA(0,1,0) MSE Error: {error:11.10}')\n",
    "\n",
    "\n",
    "error = rmse(test['close'], predictions)\n",
    "print(f'ARIMA(0,1,0) RMSE Error: {error:11.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'algo':'','name':'','date':'', 'time_frame':'','success':'','RMSE':'', 'MSE':'', 'classification':'' }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily = daily.resample('B').agg({'open':'first','high':'max',\n",
    "#                                         'low':'min', 'close':'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily['close'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(daily['close'], model='add', period=400 )\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "auto_arima(daily['close'], seasonal=True, maxiter=10000).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SARIMAX(train['close'], order=(0,1,0), seasonal_order=(1,0,1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting MSE and (P, D, Q).\n",
    "mse = 99 * (10 ** 16)\n",
    "final_P = 0\n",
    "final_D = 0\n",
    "final_Q = 0\n",
    "\n",
    "for P in range(3):\n",
    "    for Q in range(3):\n",
    "        for D in range(3):\n",
    "            try:\n",
    "                # Instantiate SARIMA model.\n",
    "                sarima = SARIMAX(endog = train['close'],\n",
    "                                 order = (0, 1, 0),              # (p, d, q)\n",
    "                                 seasonal_order = (P, D, Q, 12)) # (P, D, Q, S)\n",
    "\n",
    "                # Fit SARIMA model.\n",
    "                model = sarima.fit()\n",
    "\n",
    "                # Generate predictions based on training set.\n",
    "                # Start at time period 0 and end at 1028.\n",
    "                preds = model.predict(start=0, end=1028)\n",
    "\n",
    "                # Evaluate predictions.\n",
    "                print(f'The MSE for (1, 0, 0)x({P},{D},{Q},12) is: {mean_squared_error(train[\"close\"], preds)}')\n",
    "                \n",
    "                # Save for final report.\n",
    "                if mse > mean_squared_error(train['close'], preds):\n",
    "                    mse = mean_squared_error(train['close'], preds)\n",
    "                    final_P = P\n",
    "                    final_D = D\n",
    "                    final_Q = Q\n",
    "                \n",
    "            except:\n",
    "                print(f\"p: {P}, D: {D}, Q: {Q}\")\n",
    "                pass\n",
    "\n",
    "print(f'Our model that minimizes MSE on the training data is the SARIMA(1, 0, 0)x({final_P},{final_D},{final_Q},420).')\n",
    "print(f'This model has an MSE of {mse}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
